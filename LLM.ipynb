{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8dcafa91-d8b1-4283-9652-ee9ce7d00e34",
      "metadata": {
        "id": "8dcafa91-d8b1-4283-9652-ee9ce7d00e34"
      },
      "source": [
        "## 1st Step\n",
        "### Tokenizing the Entire story\n",
        "### Story used :- *The Verdict*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "77838885-b8a0-483c-9096-18e85c8f9b3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "77838885-b8a0-483c-9096-18e85c8f9b3f",
        "outputId": "50833510-6b88-43c0-9deb-09b4205f37ea"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'the-verdict.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3432952893.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the-verdict.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of character:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'the-verdict.txt'"
          ]
        }
      ],
      "source": [
        "with open(\"the-verdict.txt\",\"r\" , encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "483c4ea3-ce08-4834-a46b-c57b05ba9cb8",
      "metadata": {
        "id": "483c4ea3-ce08-4834-a46b-c57b05ba9cb8"
      },
      "source": [
        "### print(raw_text[:99]) is use to print first 100 characters of the sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0176eb78-29e9-41bd-82ab-1d4fd3fcdbc5",
      "metadata": {
        "id": "0176eb78-29e9-41bd-82ab-1d4fd3fcdbc5"
      },
      "source": [
        "### *We are using Regular Expresion or re library to splitting the data into tokens*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a805b33-502a-4b0c-9608-becba7642a47",
      "metadata": {
        "id": "4a805b33-502a-4b0c-9608-becba7642a47"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "text = \"Hello, world. This is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "278e3de7-07b9-476b-8592-6cc42d61f099",
      "metadata": {
        "id": "278e3de7-07b9-476b-8592-6cc42d61f099"
      },
      "outputs": [],
      "source": [
        "result = re.split(r'([,.]|\\s)', text) ## We are adding . and , as diffrent tokken\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc436094-cc32-4183-86fe-5dcb099e3702",
      "metadata": {
        "id": "dc436094-cc32-4183-86fe-5dcb099e3702"
      },
      "outputs": [],
      "source": [
        "result = [item for item in result if item.strip()] ## Removing the witespaces\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3528db81-cd7f-4072-9e46-6a21564a5251",
      "metadata": {
        "id": "3528db81-cd7f-4072-9e46-6a21564a5251"
      },
      "source": [
        "### Removing the witespaces is not always good as identation is also important in training the LLM . But here we are removing it to save spave and reduce the computation power"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cdc3662-7e3b-4167-8320-5c8921c1706b",
      "metadata": {
        "id": "6cdc3662-7e3b-4167-8320-5c8921c1706b"
      },
      "outputs": [],
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c384e84c-e450-4fee-9523-7ec143f8fe36",
      "metadata": {
        "id": "c384e84c-e450-4fee-9523-7ec143f8fe36"
      },
      "outputs": [],
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd6ea6d2-7be5-44e7-b002-440616f50007",
      "metadata": {
        "id": "bd6ea6d2-7be5-44e7-b002-440616f50007"
      },
      "outputs": [],
      "source": [
        "print(len(preprocessed))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dd08961-534e-4398-991e-00e1e69d2fbd",
      "metadata": {
        "id": "8dd08961-534e-4398-991e-00e1e69d2fbd"
      },
      "source": [
        "## 2nd Step\n",
        "### Converting tokens into token id's\n",
        "Vocabulary are created during converting tokens into token id's. In this vocabulary words are given number according to the alphabetical order . This number or integer is known as tokenID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b67f9e4-40ac-4a92-8074-83d98615e841",
      "metadata": {
        "id": "1b67f9e4-40ac-4a92-8074-83d98615e841"
      },
      "outputs": [],
      "source": [
        "all_word = sorted(set(preprocessed))\n",
        "vocab_size = len(all_word)\n",
        "\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b45007a-f418-402c-95a0-a0ad2838baa6",
      "metadata": {
        "id": "7b45007a-f418-402c-95a0-a0ad2838baa6"
      },
      "outputs": [],
      "source": [
        "vocab ={token:integer for integer,token in enumerate(all_word)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a7636c-5cb8-48b0-8671-b716adbf278c",
      "metadata": {
        "id": "c7a7636c-5cb8-48b0-8671-b716adbf278c"
      },
      "outputs": [],
      "source": [
        "for i,item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i>=50:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a0f4257-4014-473c-ba50-cf8b238951d0",
      "metadata": {
        "id": "7a0f4257-4014-473c-ba50-cf8b238951d0"
      },
      "source": [
        "### str_to_int use dictionary [vocab] to convert str to int and\n",
        "### int_to_str use reverse_dictionary to convert int to str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d399fa-12d1-4df3-80b6-dfca5fb6ed0c",
      "metadata": {
        "id": "38d399fa-12d1-4df3-80b6-dfca5fb6ed0c"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()\n",
        "                       ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5fcaea-7d6c-4c21-a6b1-93a96520c326",
      "metadata": {
        "id": "4c5fcaea-7d6c-4c21-a6b1-93a96520c326"
      },
      "outputs": [],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "            Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a11f4cc5-3c77-4856-8f8b-b2ca7dcdce4a",
      "metadata": {
        "id": "a11f4cc5-3c77-4856-8f8b-b2ca7dcdce4a"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e23737-7390-46c5-9529-b1519b641f7a",
      "metadata": {
        "id": "91e23737-7390-46c5-9529-b1519b641f7a"
      },
      "outputs": [],
      "source": [
        "# What if we use data which is not in story\n",
        "#text = \"Hello, do you like tea?\"\n",
        "#print(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daf908e9-c19b-434b-b4e2-1743b8ed79d8",
      "metadata": {
        "id": "daf908e9-c19b-434b-b4e2-1743b8ed79d8"
      },
      "source": [
        "### Dealing with Special Context Tokens\n",
        "We will modify the tokenizer to handel the unknowns words.\n",
        "### Unknown and end_of_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16bd1d41-2125-4229-b8a5-161a8c674fd4",
      "metadata": {
        "id": "16bd1d41-2125-4229-b8a5-161a8c674fd4"
      },
      "outputs": [],
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f113b9f-2e7f-4b92-ba5c-6e8e9446a575",
      "metadata": {
        "id": "3f113b9f-2e7f-4b92-ba5c-6e8e9446a575"
      },
      "outputs": [],
      "source": [
        "len(vocab.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f71d72c-63a3-437f-8b83-2469c369c0e2",
      "metadata": {
        "id": "1f71d72c-63a3-437f-8b83-2469c369c0e2"
      },
      "outputs": [],
      "source": [
        " for i,item in enumerate(list(vocab.items())[-5:]):\n",
        "     print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f1f272c-56a8-4cf1-ad7b-5d16b9ec0be0",
      "metadata": {
        "id": "9f1f272c-56a8-4cf1-ad7b-5d16b9ec0be0"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV2:\n",
        "\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.?!:;_\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [item if item in self.str_to_int\n",
        "                       else \"<|unk|>\" for item in preprocessed]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "095e2199-4425-4409-90b2-fd4fe7dc27e6",
      "metadata": {
        "id": "095e2199-4425-4409-90b2-fd4fe7dc27e6"
      },
      "outputs": [],
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "214353d2-c578-4e4d-9526-1f04479eb381",
      "metadata": {
        "id": "214353d2-c578-4e4d-9526-1f04479eb381"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f7bf02-9fb3-40e4-8a6e-5cd2720cf60e",
      "metadata": {
        "id": "42f7bf02-9fb3-40e4-8a6e-5cd2720cf60e"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc2e6cc9-4f91-4d19-b411-157f28c6b14f",
      "metadata": {
        "id": "cc2e6cc9-4f91-4d19-b411-157f28c6b14f"
      },
      "source": [
        "## There are also other tokens such as the following\n",
        "### [BOS] (beginning of sequence):\n",
        "The token mark the start of a text.\n",
        "### [EOS] (end of sequence):\n",
        "The token is positioned at the end of a text.\n",
        "### [PAD] (padding):\n",
        "while training LLMs with batch size larger than one the batch might contain texts of varying lengths."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90e3000b-c950-4dac-bbfb-400309dc16db",
      "metadata": {
        "id": "90e3000b-c950-4dac-bbfb-400309dc16db"
      },
      "source": [
        "## BYTE PAIR ENCODING (BPE)\n",
        "In GPT model they don't use unknown they use BYTE PAIR ENCODING, in which the words are broken into subwords and these subwords will become tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizers Types\n",
        "1. Word based tokenizer ['My','name','snehal'] Problem-OOV words.\n",
        "2. Character based tokenizer ['M','y','n','a','m','e','S','n','e','h','a','l'] No OOV words problem, meaning will be lost , too many tokens\n",
        "3. Sub word based tokenizer\n",
        "   ### Rules -\n",
        "   1. Do not split frequentlu used words into smaller subwords.\n",
        "   2. Split the rare word into smaller meaningful subwords.\n",
        "      \n",
        "      eg - \"boy\" should not be split\n",
        "           \"boys\" should be split into \"boy\" and \"s\"\n",
        "\n",
        "BYTE PAIR ALGO IS ALSO A SUB WORD TOKENIZER\n",
        "   "
      ],
      "metadata": {
        "id": "73005048-33af-4d9b-94fb-93a33d9d0140"
      },
      "id": "73005048-33af-4d9b-94fb-93a33d9d0140"
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version: \", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "id": "dgZr67ly3uhh"
      },
      "id": "dgZr67ly3uhh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "UYTplW7v3w2b"
      },
      "id": "UYTplW7v3w2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello do you know me? <|endoftext|> I don't knowyou.\"\n",
        ")\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "yNw5AsK_30VY",
        "outputId": "f40da7f8-7944-43ca-ace5-f056592186ea"
      },
      "id": "yNw5AsK_30VY",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1501676223.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"Hello do you know me? <|endoftext|> I don't knowyou.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mintegers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_special\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"<|endoftext|>\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintegers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "id": "sDKuHUol31mL"
      },
      "id": "sDKuHUol31mL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "integers = tokenizer.encode(\"Akwifns dfs\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "Dm1oYRlS33Ew",
        "outputId": "2c9be109-063b-49b4-ad11-e67a89cb057e"
      },
      "id": "Dm1oYRlS33Ew",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2889839136.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mintegers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Akwifns dfs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintegers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstrings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintegers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "integers = tokenizer.encode(\"Megamasterfull world with no meaning have zero sense\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "id": "_89jfpP634rv"
      },
      "id": "_89jfpP634rv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Input - Target pairs\n",
        "Autoregresive"
      ],
      "metadata": {
        "id": "nLZEQ4rc36Nh"
      },
      "id": "nLZEQ4rc36Nh"
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\",\"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "UtZT9WAw38lD",
        "outputId": "7f6818e9-82ca-41a2-c06e-2017d6839b38"
      },
      "id": "UtZT9WAw38lD",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'the-verdict.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3386566809.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the-verdict.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menc_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'the-verdict.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "DMRbhM6I39mr"
      },
      "id": "DMRbhM6I39mr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y: {y}\")"
      ],
      "metadata": {
        "id": "9MeE01mE4oO3"
      },
      "id": "9MeE01mE4oO3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context,\"--->\",desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "eJZShmJ74o_T",
        "outputId": "751cd20e-c570-4c16-f4d8-9d00fc3bb539"
      },
      "id": "eJZShmJ74o_T",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'context_size' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3366347147.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdesired\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"--->\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdesired\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'context_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything left of the arrow (--->) refers to the input an LLM would receive, and the tokenID on the right side of the arrow is the target ID that the LLM will predict"
      ],
      "metadata": {
        "id": "xf0pQvBc4qx5"
      },
      "id": "xf0pQvBc4qx5"
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context),\"--->\",tokenizer.decode([desired]))"
      ],
      "metadata": {
        "id": "FFar30oh4t9W"
      },
      "id": "FFar30oh4t9W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to create two tensors one input tensors and one output tensors\n",
        "### Dataset and DataLoader"
      ],
      "metadata": {
        "id": "REZOLtym4vDw"
      },
      "id": "REZOLtym4vDw"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "g02TKOhM4xCL"
      },
      "id": "g02TKOhM4xCL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                        stride=128, shuffle=True, drop_last=True,\n",
        "                        num_workers=0):\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "T7SzLwR_4zfZ"
      },
      "id": "T7SzLwR_4zfZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ],
      "metadata": {
        "id": "GS3DSbqF403A"
      },
      "id": "GS3DSbqF403A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PyTorch version:\", torch.__version__)\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "id": "N9xzaohB42a6"
      },
      "id": "N9xzaohB42a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "ZOnkLPpo43xW",
        "outputId": "54d46f92-2264-4195-b6f7-68b939f3ae49"
      },
      "id": "ZOnkLPpo43xW",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data_iter' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-143451283.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msecond_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_iter' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATING TOKEN EMBEDDINGS"
      ],
      "metadata": {
        "id": "ZTyv2Sio45oS"
      },
      "id": "ZTyv2Sio45oS"
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ],
      "metadata": {
        "id": "MUiayPomTnEP"
      },
      "id": "MUiayPomTnEP",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We are using the embedding size of 3"
      ],
      "metadata": {
        "id": "zUk3ElJpT5H8"
      },
      "id": "zUk3ElJpT5H8"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "qCEi_9XZTxJr"
      },
      "id": "qCEi_9XZTxJr",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XnWnTUEWGFJ",
        "outputId": "4a949f84-8d71-4b4f-cb89-bc1499cfe4e2"
      },
      "id": "8XnWnTUEWGFJ",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### These weights are random they are normalized during LLM training"
      ],
      "metadata": {
        "id": "hK9Gy3KeaCEt"
      },
      "id": "hK9Gy3KeaCEt"
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vorCOS5oaQ56",
        "outputId": "29205d57-d3f2-48ab-e9a6-47a57d110d34"
      },
      "id": "vorCOS5oaQ56",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG3cF_dCaqFq",
        "outputId": "d213edc9-48cb-49a9-97c7-926a86707e51"
      },
      "id": "eG3cF_dCaqFq",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tdx = torch.tensor([2, 3, 1])"
      ],
      "metadata": {
        "id": "YKIp5ZxWn2ko"
      },
      "id": "YKIp5ZxWn2ko",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 4\n",
        "output_dim = 5\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "RwaPx2BopNYu"
      },
      "id": "RwaPx2BopNYu",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGJ-VPPcqTVQ",
        "outputId": "71acdc84-23c4-4f8f-b9e3-152f8780d03d"
      },
      "id": "zGJ-VPPcqTVQ",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  1.5810],\n",
            "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015],\n",
            "        [ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(tdx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f3SUELTq1pX",
        "outputId": "58d4325a-d09e-4d2b-a8a0-a8629e397bc6"
      },
      "id": "3f3SUELTq1pX",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n",
            "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POSITIONAL EMBEDDING (ENCODING WORD POSITIONS)\n",
        "We are going to assume that the token IDs were created by the BPE tokenizer, which hasvocabolary size of almost 50257  tokens"
      ],
      "metadata": {
        "id": "DtrgCxHXrFVY"
      },
      "id": "DtrgCxHXrFVY"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "kfneYTGV23t4",
        "outputId": "46622b8f-6b3a-44af-b028-7d3c3e64fd7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "id": "kfneYTGV23t4",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-492469095.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50257\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtoken_embedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lJoHEdLW4UeE"
      },
      "id": "lJoHEdLW4UeE",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, target = next(data_iter)"
      ],
      "metadata": {
        "id": "5PaLR7_P36xB",
        "outputId": "b40f8d06-eb30-4c22-bc1c-bcd2975c24cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "id": "5PaLR7_P36xB",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'create_dataloader_v1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-121185920.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m dataloader = create_dataloader_v1(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mraw_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_dataloader_v1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\\n\" , inputs)\n",
        "print(\"\\nInput shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "id": "YAkQTh9k5Byo",
        "outputId": "4ca9f6ff-9f5e-49f9-ea08-198b493a9d4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "id": "YAkQTh9k5Byo",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'inputs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2388971034.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Token IDs:\\n\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nInput shape:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embedding = token_embedding_layer(inputs)\n",
        "print(token_embedding.shape)"
      ],
      "metadata": {
        "id": "Ucjhuzfp5gNr",
        "outputId": "7f06b1d0-52c0-4ad8-f57e-650e6d082acb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "id": "Ucjhuzfp5gNr",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'token_embedding_layer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2672354548.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_embedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'token_embedding_layer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ],
      "metadata": {
        "id": "_2jcf0Vf6FQt",
        "outputId": "2675014f-7126-4097-bdab-1b78b86a6870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "id": "_2jcf0Vf6FQt",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2727216359.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontext_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpos_embedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embedding = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embedding.shape)"
      ],
      "metadata": {
        "id": "CVcjB_kw6zWg",
        "outputId": "27f804ce-5489-45cf-b807-2ebac823938b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "id": "CVcjB_kw6zWg",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pos_embedding_layer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2539566841.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpos_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_embedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pos_embedding_layer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89], # Your (x^1)\n",
        "    [0.55, 0.87, 0.66], # journey (x^2)\n",
        "    [0.57, 0.85, 0.64], # start (x^3)\n",
        "    [0.22, 0.58, 0.33], # with (x^4)\n",
        "    [0.77, 0.25, 0.10], # one (x^5)\n",
        "    [0.05, 0.80, 0.55] # step (x^6)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "ipqoMpsP7Ken"
      },
      "id": "ipqoMpsP7Ken",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "  attn_scores_2[i] = torch.dot(x_i, query)\n",
        "\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez3n_fsspxpR",
        "outputId": "f614b24c-1071-4fdd-f7a0-adda63ff755d"
      },
      "id": "ez3n_fsspxpR",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2_temp = attn_scores_2 / attn_scores_2.sum()\n",
        "\n",
        "print(\"Attention weights :\",attn_weights_2_temp)\n",
        "print(\"Sum :\", attn_weights_2_temp.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R47oJCKFrIsd",
        "outputId": "7c168927-da0d-44cf-b64d-113e4ae3dc8b"
      },
      "id": "R47oJCKFrIsd",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights : tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum : tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_naive(x):\n",
        "  return torch.exp(x) / torch.exp(x).sum()\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "\n",
        "print(\"Attention weights :\",attn_weights_2_naive)\n",
        "print(\"Sum :\", attn_weights_2_naive.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU4U2ArzrqRo",
        "outputId": "301b4d0d-4bd7-49d7-963e-b4db94fc65d1"
      },
      "id": "rU4U2ArzrqRo",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights : tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum : tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "print(\"Attention weights :\", attn_weights_2)\n",
        "print(\"Sum : \", attn_weights_2.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l-j-ctBvsrK",
        "outputId": "8a7ab3a4-cbe4-4828-a9e1-e9695f8ff1aa"
      },
      "id": "8l-j-ctBvsrK",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights : tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum :  tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]\n",
        "\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i, x_i in enumerate(inputs):\n",
        "  context_vec_2 += attn_weights_2[i]* x_i\n",
        "\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USkD2GiOsRUb",
        "outputId": "2509f9ae-30a3-40c3-cf6a-3c582e5c3cea"
      },
      "id": "USkD2GiOsRUb",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attnetion scores of all the inputs"
      ],
      "metadata": {
        "id": "_u2CQtfdxxcD"
      },
      "id": "_u2CQtfdxxcD"
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.empty(6,6)\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "  for j, x_j in enumerate(inputs):\n",
        "    attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4MNuVaiwGEf",
        "outputId": "564053ec-6a2c-43aa-8392-e3c14a40c8f5"
      },
      "id": "y4MNuVaiwGEf",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = inputs @ inputs.T\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdNxK5Hsxp5A",
        "outputId": "4566d93d-d521-4a92-face-96cb21c68ed4"
      },
      "id": "PdNxK5Hsxp5A",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the Attention weigths of all the inputs"
      ],
      "metadata": {
        "id": "GPjv_xHRyMBz"
      },
      "id": "GPjv_xHRyMBz"
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMrK-09yyhcs",
        "outputId": "c86282fb-e000-4e01-bdb8-df019ca82b5c"
      },
      "id": "pMrK-09yyhcs",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
              "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
              "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
              "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
              "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
              "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By setting dim=-1 we are instructing the softmax function to apply the normalization along the last dimention of the attn_scores tensor"
      ],
      "metadata": {
        "id": "rnEBkB6zyqwY"
      },
      "id": "rnEBkB6zyqwY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the Context vector"
      ],
      "metadata": {
        "id": "lkTk0ten1Eyj"
      },
      "id": "lkTk0ten1Eyj"
    },
    {
      "cell_type": "code",
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ4YnAwyzn09",
        "outputId": "9dc11eb8-d4b6-492b-d4e4-549eb86b4312"
      },
      "id": "XQ4YnAwyzn09",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENTING SELF ATTENTION WITH TRAINABLE WEIGHTS"
      ],
      "metadata": {
        "id": "AG1ncISB0l-5"
      },
      "id": "AG1ncISB0l-5"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89], # Your (x^1)\n",
        "    [0.55, 0.87, 0.66], # journey (x^2)\n",
        "    [0.57, 0.85, 0.64], # start (x^3)\n",
        "    [0.22, 0.58, 0.33], # with (x^4)\n",
        "    [0.77, 0.25, 0.10], # one (x^5)\n",
        "    [0.05, 0.80, 0.55] # step (x^6)\n",
        "    ])"
      ],
      "metadata": {
        "id": "TpNJN7wH0n8M"
      },
      "id": "TpNJN7wH0n8M",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In models the inputs and output dimentions are always same."
      ],
      "metadata": {
        "id": "pDmFX4bj2KSp"
      },
      "id": "pDmFX4bj2KSp"
    },
    {
      "cell_type": "code",
      "source": [
        "x_2 = inputs[1]\n",
        "d_in = inputs.shape[1]\n",
        "d_out = 2"
      ],
      "metadata": {
        "id": "ZQp9f5uH0xm-"
      },
      "id": "ZQp9f5uH0xm-",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
      ],
      "metadata": {
        "id": "r2RCjmer1QIZ"
      },
      "id": "r2RCjmer1QIZ",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(W_query)"
      ],
      "metadata": {
        "id": "PfGsLU4G1zfn",
        "outputId": "11f6b5d8-da02-4490-edf5-79d048bee84c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PfGsLU4G1zfn",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(W_key)"
      ],
      "metadata": {
        "id": "VnqoqiZx2BAD",
        "outputId": "c32936ef-2ea0-43b2-eaec-0e3737f833af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VnqoqiZx2BAD",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.1366, 0.1025],\n",
            "        [0.1841, 0.7264],\n",
            "        [0.3153, 0.6871]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(W_value)"
      ],
      "metadata": {
        "id": "0Yj9FSme2CoT",
        "outputId": "62771b71-b346-4905-b9e4-d8d40cbb8bc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0Yj9FSme2CoT",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.0756, 0.1966],\n",
            "        [0.3164, 0.4017],\n",
            "        [0.1186, 0.8274]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "print(query_2,\"\\n\",key_2,\"\\n\",value_2)"
      ],
      "metadata": {
        "id": "w1PMlDLu2EXX",
        "outputId": "c7a1d8a1-0ebc-4bd6-a77e-19096bfb9fd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "w1PMlDLu2EXX",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551]) \n",
            " tensor([0.4433, 1.1419]) \n",
            " tensor([0.3951, 1.0037])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries = inputs @ W_query\n",
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "\n",
        "print(f\"Query.shape: {queries.shape}\")\n",
        "print(f\"Keys.shape: {keys.shape}\")\n",
        "print(f\"Values.shape: {values.shape}\")"
      ],
      "metadata": {
        "id": "mKQ9Dz-_3d1R",
        "outputId": "dfc2589c-26c8-44c0-8fc6-c3b68b927a4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mKQ9Dz-_3d1R",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query.shape: torch.Size([6, 2])\n",
            "Keys.shape: torch.Size([6, 2])\n",
            "Values.shape: torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Query matrix: {queries}\\n\")\n",
        "print(f\"Keys matrix: {keys}\\n\")\n",
        "print(f\"Values matrix: {values}\")"
      ],
      "metadata": {
        "id": "vb6B3_kc4zqn",
        "outputId": "7e2be78a-79f4-4a4d-975e-5440985e1c9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vb6B3_kc4zqn",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query matrix: tensor([[0.2309, 1.0966],\n",
            "        [0.4306, 1.4551],\n",
            "        [0.4300, 1.4343],\n",
            "        [0.2355, 0.7990],\n",
            "        [0.2983, 0.6565],\n",
            "        [0.2568, 1.0533]])\n",
            "\n",
            "Keys matrix: tensor([[0.3669, 0.7646],\n",
            "        [0.4433, 1.1419],\n",
            "        [0.4361, 1.1156],\n",
            "        [0.2408, 0.6706],\n",
            "        [0.1827, 0.3292],\n",
            "        [0.3275, 0.9642]])\n",
            "\n",
            "Values matrix: tensor([[0.1855, 0.8812],\n",
            "        [0.3951, 1.0037],\n",
            "        [0.3879, 0.9831],\n",
            "        [0.2393, 0.5493],\n",
            "        [0.1492, 0.3346],\n",
            "        [0.3221, 0.7863]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys_2 = keys[1]\n",
        "\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ],
      "metadata": {
        "id": "DzrLnzuq5OVL",
        "outputId": "f64e5dd5-8a34-4bc8-8c11-fe5b45119a65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DzrLnzuq5OVL",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores_2 = query_2 @ keys.T\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "id": "_Hx1QBf3qR9i",
        "outputId": "e5d118ba-2792-4ad7-ee6c-df6f38af73ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_Hx1QBf3qR9i",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = queries @ keys.T\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "id": "yjbkLg0Rqs1a",
        "outputId": "7da2e113-964f-4877-f1e4-f78819d2e83d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yjbkLg0Rqs1a",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
            "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
            "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
            "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
            "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
            "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling"
      ],
      "metadata": {
        "id": "Lr3C6_hLEhH6"
      },
      "id": "Lr3C6_hLEhH6"
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)\n",
        "print(d_k)"
      ],
      "metadata": {
        "id": "sVut7UwWqyjG",
        "outputId": "2d470ef0-e4c9-4f9e-d59f-b5910ce1580a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sVut7UwWqyjG",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = keys.shape[-1]\n",
        "attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
        "print(attn_weights)\n",
        "print(d_k)"
      ],
      "metadata": {
        "id": "WINShRHOCaM_",
        "outputId": "02173b77-8567-4644-fc0c-e1bceb35cb00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WINShRHOCaM_",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
            "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
            "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
            "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
            "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
            "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context vector"
      ],
      "metadata": {
        "id": "a64gapPIQmAl"
      },
      "id": "a64gapPIQmAl"
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "id": "EXcXG4QGDIyr",
        "outputId": "85e7749a-976e-44c1-d4f0-33ce3c4b4f6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EXcXG4QGDIyr",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENTING A COMPACT SELF ATTENTION PYTHON CLASS"
      ],
      "metadata": {
        "id": "6PhCJauAFPEO"
      },
      "id": "6PhCJauAFPEO"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "  def forward(self, x):\n",
        "    querys = x @ self.W_query\n",
        "    keys = x @ self.W_key\n",
        "    values = x @ self.W_value\n",
        "\n",
        "    attn_score = querys @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "nMWW8MxiS9uc"
      },
      "id": "nMWW8MxiS9uc",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))"
      ],
      "metadata": {
        "id": "6e2IqDCwWbZU",
        "outputId": "5a3d5658-df5d-4355-992c-baf25fe53f79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6e2IqDCwWbZU",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Linear(d_in, d_out , bias = qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out , bias = qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out , bias = qkv_bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    querys = self.W_query(x)\n",
        "    keys = self.W_key(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    attn_score = querys @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "vrDpikBQWnPi"
      },
      "id": "vrDpikBQWnPi",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "id": "WSM0Sdhbw8fs",
        "outputId": "ee9d8f53-e112-4218-b96b-121332f3c831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WSM0Sdhbw8fs",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0938,  0.0499],\n",
            "        [-0.0985,  0.0460],\n",
            "        [-0.0982,  0.0462],\n",
            "        [-0.0901,  0.0535],\n",
            "        [-0.0883,  0.0552],\n",
            "        [-0.0934,  0.0503]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HIDING FUTURE WORDS WITH CASUAL ATTENTION"
      ],
      "metadata": {
        "id": "85CqfW4cxJlm"
      },
      "id": "85CqfW4cxJlm"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89], # Your (x^1)\n",
        "    [0.55, 0.87, 0.66], # journey (x^2)\n",
        "    [0.57, 0.85, 0.64], # start (x^3)\n",
        "    [0.22, 0.58, 0.33], # with (x^4)\n",
        "    [0.77, 0.25, 0.10], # one (x^5)\n",
        "    [0.05, 0.80, 0.55] # step (x^6)\n",
        "    ])"
      ],
      "metadata": {
        "id": "7GY7BIfeSmVJ"
      },
      "id": "7GY7BIfeSmVJ",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "id": "zIkkCdtPR1Gv",
        "outputId": "3db27080-eaec-4091-d805-720c77bde050",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zIkkCdtPR1Gv",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "mask_simple  = torch.tril(torch.ones(context_length, context_length))\n",
        "print(mask_simple)"
      ],
      "metadata": {
        "id": "qmOuKE_pR1J7",
        "outputId": "6a9640e7-f85d-4a94-f7db-1253a53e1b3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qmOuKE_pR1J7",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_simple = attn_weights*mask_simple\n",
        "print(masked_simple)"
      ],
      "metadata": {
        "id": "oj0AwmRrTCzP",
        "outputId": "980c1d01-e8d1-4b45-8c93-213f50ba1957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oj0AwmRrTCzP",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "print(masked_simple_norm)"
      ],
      "metadata": {
        "id": "UGZzdfdNTtzv",
        "outputId": "870b0b46-27dc-4c5d-ffb8-0d2922ce9e49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UGZzdfdNTtzv",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Better Way"
      ],
      "metadata": {
        "id": "oXOgaFsuU6wk"
      },
      "id": "oXOgaFsuU6wk"
    },
    {
      "cell_type": "code",
      "source": [
        "print(attn_scores)"
      ],
      "metadata": {
        "id": "KIEpf1OXUDSA",
        "outputId": "39414fe9-25d0-47d0-d40e-9e3dd9958d2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KIEpf1OXUDSA",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
            "        [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
            "        [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
            "        [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
            "        [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
            "        [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ],
      "metadata": {
        "id": "iVs_dDCKU-wd",
        "outputId": "9829fd8c-5b4c-475f-d0f0-820dc9b1f296",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "iVs_dDCKU-wd",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "id": "l-Hn6M3GVRtv",
        "outputId": "e86a7a15-0f99-4a02-883f-0cc0a964e674",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "l-Hn6M3GVRtv",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MASKING ADDITION ATTENTION WEIGHTS USING DROPOUT"
      ],
      "metadata": {
        "id": "4kQoHajRVmFk"
      },
      "id": "4kQoHajRVmFk"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5)\n",
        "example = torch.ones(6,6)\n",
        "print(dropout(example))"
      ],
      "metadata": {
        "id": "lIKygSd1OjAD",
        "outputId": "84eda943-d7ae-4ced-dd5e-02bac4e399f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lIKygSd1OjAD",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "print(dropout(attn_weights))"
      ],
      "metadata": {
        "id": "2izQUTieOvmn",
        "outputId": "8c9ad656-18c5-4a24-db07-a41bd83369ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2izQUTieOvmn",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMPLEMENTING A COMPACT CAUSAL ATTENTION CLASS"
      ],
      "metadata": {
        "id": "hxistjgiPWIA"
      },
      "id": "hxistjgiPWIA"
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "id": "3aMx5HpZP0iY",
        "outputId": "11528737-8902-444f-be61-b95a2f84f535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3aMx5HpZP0iY",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_in, d_out, context_lenght, dropout,\n",
        "               qkv_bias = False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_lenght, context_lenght), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape # Here b is the batch size\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(1, 2) #we only need  num_tokens and d_in\n",
        "    attn_scores.masked_fill(\n",
        "        self.mask.bool() [:num_tokens, :num_tokens], -torch.inf)\n",
        "    attn_weights =torch.softmax(\n",
        "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "    )\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "-8iqZzgTQDtA"
      },
      "id": "-8iqZzgTQDtA",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(d_in)"
      ],
      "metadata": {
        "id": "EUHm5wEK3eKc",
        "outputId": "06a48df5-8fc4-48e7-f89a-f0031bb544ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EUHm5wEK3eKc",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(d_out)"
      ],
      "metadata": {
        "id": "swTBNU6M3gW3",
        "outputId": "473ad34e-f7c8-4623-ec41-2e1cd2f968ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "swTBNU6M3gW3",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_lenght = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "context_vec = ca(batch)\n",
        "print(\"Context_vec.shape:\", context_vec.shape)"
      ],
      "metadata": {
        "id": "XxfTFzRh2ZzQ",
        "outputId": "6aa6bd91-71eb-4d27-8bf5-b4b047be535e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XxfTFzRh2ZzQ",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context_vec.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(context_vec)"
      ],
      "metadata": {
        "id": "swkZn-Hk25UA",
        "outputId": "4ccc0b88-ba31-4e2a-b724-d54f2552fe66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "swkZn-Hk25UA",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.5337, -0.1051],\n",
            "         [-0.5323, -0.1080],\n",
            "         [-0.5323, -0.1079],\n",
            "         [-0.5297, -0.1076],\n",
            "         [-0.5311, -0.1066],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.5337, -0.1051],\n",
            "         [-0.5323, -0.1080],\n",
            "         [-0.5323, -0.1079],\n",
            "         [-0.5297, -0.1076],\n",
            "         [-0.5311, -0.1066],\n",
            "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXTENDING SINGLE HEAD ATTENTION TO MULTI-HEAD ATTENTION"
      ],
      "metadata": {
        "id": "U6tIP53T3vXN"
      },
      "id": "U6tIP53T3vXN"
    },
    {
      "cell_type": "code",
      "source": [
        "## USing MultiHeadAttentionWrapper\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_lenght, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList(\n",
        "        [CausalAttention(d_in, d_out, context_lenght, dropout, qkv_bias)\n",
        "        for _ in range(num_heads)]\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "jNGuTgU25aAN"
      },
      "id": "jNGuTgU25aAN",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "id": "KDWYKB2GBgM-",
        "outputId": "7223ffd2-d943-4ec7-c8cc-41d815ed12cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KDWYKB2GBgM-",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "d_in, d_out = 3, 2\n",
        "mha = MultiHeadAttentionWrapper(d_in, d_out, context_lenght, 0.0, num_heads=2)\n",
        "context_vec = mha(batch)\n",
        "print(context_vec)\n",
        "print(\"context_vec shape: \",context_vec.shape)"
      ],
      "metadata": {
        "id": "id6-SSblCCCx",
        "outputId": "f63fb4d5-5163-4bbf-8419-3154100c898a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "id6-SSblCCCx",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.5337, -0.1051,  0.5085,  0.3508],\n",
            "         [-0.5323, -0.1080,  0.5084,  0.3508],\n",
            "         [-0.5323, -0.1079,  0.5084,  0.3506],\n",
            "         [-0.5297, -0.1076,  0.5074,  0.3471],\n",
            "         [-0.5311, -0.1066,  0.5076,  0.3446],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.5337, -0.1051,  0.5085,  0.3508],\n",
            "         [-0.5323, -0.1080,  0.5084,  0.3508],\n",
            "         [-0.5323, -0.1079,  0.5084,  0.3506],\n",
            "         [-0.5297, -0.1076,  0.5074,  0.3471],\n",
            "         [-0.5311, -0.1066,  0.5076,  0.3446],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
            "context_vec shape:  torch.Size([2, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPLEMENTING MULTI-HEAD ATTENTION WITH WEIGHT SPLITS"
      ],
      "metadata": {
        "id": "vT8E0MpWH4fB"
      },
      "id": "vT8E0MpWH4fB"
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert (d_out % num_heads == 0), \\\n",
        "    \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads # Reduce the projection dim to match the desired output dim\n",
        "\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "        \"mask\",\n",
        "        torch.triu(torch.ones(context_length, context_length),\n",
        "                   diagonal=1)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "\n",
        "    keys = self.W_key(x) # Shape: b, num_tokens, d_out\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    # We implicitly split the matrix by adding a 'num_heads dimension'\n",
        "    # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "    keys = keys.transpose(1, 2)\n",
        "    queries = queries.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    # Computing  the dot product attebtoin with a casulat mask\n",
        "    attn_scores = queries @ keys.transpose(2,3)\n",
        "\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    # Us ethe mask to fill the attention scores\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "    context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "    # Combining heads, where self.d_out = self.num_heads * self.head_dim\n",
        "    context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "    context_vec = self.out_proj(context_vec)\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "REFP6sWFJy0K"
      },
      "id": "REFP6sWFJy0K",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],\n",
        "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],\n",
        "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "batch.size, context_length, d_in = batch.shape\n",
        "d_out = 6\n",
        "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape :\",context_vecs.shape)"
      ],
      "metadata": {
        "id": "85PwPLNz2k_3",
        "outputId": "6e307279-c379-486c-9c72-711c141069d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "85PwPLNz2k_3",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
            "\n",
            "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape : torch.Size([2, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZdcFIj9v85Gv"
      },
      "id": "ZdcFIj9v85Gv",
      "execution_count": 40,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}